{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logic_problem import LogicProblem\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def __init__(self, key_size, loss_fn=None, embed_dim=None, num_FIN_layers=None):\n",
    "# loss = torch.nn.CrossEntropyLoss\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "logic = LogicProblem(key_size=16, loss_fn=loss_fn, embed_dim=64, num_FIN_layers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 7.674966335296631\n",
      "Epoch 1 loss: 7.674715518951416\n",
      "Epoch 2 loss: 7.67482328414917\n",
      "Epoch 3 loss: 7.675973415374756\n",
      "Epoch 4 loss: 7.675468921661377\n",
      "Epoch 5 loss: 7.674876689910889\n",
      "Epoch 6 loss: 7.675151824951172\n",
      "Epoch 7 loss: 7.675296783447266\n",
      "Epoch 8 loss: 7.674158573150635\n",
      "Epoch 9 loss: 7.675835132598877\n",
      "Epoch 10 loss: 7.67656135559082\n",
      "Epoch 11 loss: 7.67563009262085\n",
      "Epoch 12 loss: 7.674076557159424\n",
      "Epoch 13 loss: 7.676232814788818\n",
      "Epoch 14 loss: 7.675333499908447\n",
      "Epoch 15 loss: 7.675070285797119\n",
      "Epoch 16 loss: 7.674407958984375\n",
      "Epoch 17 loss: 7.675303936004639\n",
      "Epoch 18 loss: 7.675279140472412\n",
      "Epoch 19 loss: 7.674564838409424\n",
      "Epoch 20 loss: 7.674299240112305\n",
      "Epoch 21 loss: 7.675381183624268\n",
      "Epoch 22 loss: 7.676049709320068\n",
      "Epoch 23 loss: 7.674849033355713\n",
      "Epoch 24 loss: 7.675657272338867\n",
      "Epoch 25 loss: 7.676366806030273\n",
      "Epoch 26 loss: 7.675355434417725\n",
      "Epoch 27 loss: 7.6753997802734375\n",
      "Epoch 28 loss: 7.675357818603516\n",
      "Epoch 29 loss: 7.675411224365234\n",
      "Epoch 30 loss: 7.674253940582275\n",
      "Epoch 31 loss: 7.674305438995361\n",
      "Epoch 32 loss: 7.675894260406494\n",
      "Epoch 33 loss: 7.675212860107422\n",
      "Epoch 34 loss: 7.675909042358398\n",
      "Epoch 35 loss: 7.674484729766846\n",
      "Epoch 36 loss: 7.674370288848877\n",
      "Epoch 37 loss: 7.675481796264648\n",
      "Epoch 38 loss: 7.6751885414123535\n",
      "Epoch 39 loss: 7.6740403175354\n",
      "Epoch 40 loss: 7.675769329071045\n",
      "Epoch 41 loss: 7.676748752593994\n",
      "Epoch 42 loss: 7.675074100494385\n",
      "Epoch 43 loss: 7.675872802734375\n",
      "Epoch 44 loss: 7.675516605377197\n",
      "Epoch 45 loss: 7.675518035888672\n",
      "Epoch 46 loss: 7.6753668785095215\n",
      "Epoch 47 loss: 7.676372528076172\n",
      "Epoch 48 loss: 7.675802230834961\n",
      "Epoch 49 loss: 7.67477560043335\n",
      "Epoch 50 loss: 7.675734043121338\n",
      "Epoch 51 loss: 7.675533294677734\n",
      "Epoch 52 loss: 7.675258636474609\n",
      "Epoch 53 loss: 7.675671100616455\n",
      "Epoch 54 loss: 7.675282001495361\n",
      "Epoch 55 loss: 7.675041198730469\n",
      "Epoch 56 loss: 7.6754069328308105\n",
      "Epoch 57 loss: 7.675520420074463\n",
      "Epoch 58 loss: 7.675490856170654\n",
      "Epoch 59 loss: 7.675405502319336\n",
      "Epoch 60 loss: 7.675224304199219\n",
      "Epoch 61 loss: 7.675220489501953\n",
      "Epoch 62 loss: 7.675300598144531\n",
      "Epoch 63 loss: 7.675632953643799\n",
      "Epoch 64 loss: 7.675257205963135\n",
      "Epoch 65 loss: 7.675597667694092\n",
      "Epoch 66 loss: 7.675233840942383\n",
      "Epoch 67 loss: 7.675513744354248\n",
      "Epoch 68 loss: 7.675548553466797\n",
      "Epoch 69 loss: 7.675405979156494\n",
      "Epoch 70 loss: 7.6751389503479\n",
      "Epoch 71 loss: 7.675409317016602\n",
      "Epoch 72 loss: 7.675316333770752\n",
      "Epoch 73 loss: 7.675189971923828\n",
      "Epoch 74 loss: 7.675260066986084\n",
      "Epoch 75 loss: 7.67488431930542\n",
      "Epoch 76 loss: 7.675151824951172\n",
      "Epoch 77 loss: 7.675591945648193\n",
      "Epoch 78 loss: 7.675436019897461\n",
      "Epoch 79 loss: 7.675649642944336\n",
      "Epoch 80 loss: 7.675634860992432\n",
      "Epoch 81 loss: 7.675386905670166\n",
      "Epoch 82 loss: 7.674990177154541\n",
      "Epoch 83 loss: 7.675331115722656\n",
      "Epoch 84 loss: 7.675348281860352\n",
      "Epoch 85 loss: 7.675157070159912\n",
      "Epoch 86 loss: 7.675262451171875\n",
      "Epoch 87 loss: 7.675118923187256\n",
      "Epoch 88 loss: 7.675037860870361\n",
      "Epoch 89 loss: 7.675394535064697\n",
      "Epoch 90 loss: 7.675232410430908\n",
      "Epoch 91 loss: 7.675258636474609\n",
      "Epoch 92 loss: 7.675248622894287\n",
      "Epoch 93 loss: 7.675215244293213\n",
      "Epoch 94 loss: 7.674952983856201\n",
      "Epoch 95 loss: 7.674990177154541\n",
      "Epoch 96 loss: 7.67488431930542\n",
      "Epoch 97 loss: 7.675554275512695\n",
      "Epoch 98 loss: 7.675410747528076\n",
      "Epoch 99 loss: 7.6754889488220215\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogicModelMA(\n",
       "  (projection_prop): Sequential(\n",
       "    (0): Linear(in_features=51, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (projection_op): Sequential(\n",
       "    (0): Linear(in_features=19, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (projection_goal): Sequential(\n",
       "    (0): Linear(in_features=51, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (MA_list): ModuleList(\n",
       "    (0): MixingAttention(\n",
       "      (MHA): ModuleDict(\n",
       "        (goal): ModuleDict(\n",
       "          (goal): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "          (op): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "          (prop): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (op): ModuleDict(\n",
       "          (goal): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "          (op): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "          (prop): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (prop): ModuleDict(\n",
       "          (goal): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "          (op): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "          (prop): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (LN_Z): ModuleDict(\n",
       "        (goal): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (op): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (prop): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (FF_A): ModuleDict(\n",
       "        (goal): Sequential(\n",
       "          (0): Linear(in_features=192, out_features=384, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=384, out_features=192, bias=True)\n",
       "        )\n",
       "        (op): Sequential(\n",
       "          (0): Linear(in_features=192, out_features=384, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=384, out_features=192, bias=True)\n",
       "        )\n",
       "        (prop): Sequential(\n",
       "          (0): Linear(in_features=192, out_features=384, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=384, out_features=192, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (W0_A): ParameterDict(\n",
       "          (goal): Parameter containing: [torch.FloatTensor of size 192x64]\n",
       "          (op): Parameter containing: [torch.FloatTensor of size 192x64]\n",
       "          (prop): Parameter containing: [torch.FloatTensor of size 192x64]\n",
       "      )\n",
       "      (LN_out): ModuleDict(\n",
       "        (goal): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (op): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (prop): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dp): Linear(in_features=64, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def train(self, epochs, batch_size):\n",
    "logic.train(epochs=100, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n'prop_mtx': prop_mtx,\\n'op_mtx': op_mtx,\\n'goal_mtx': goal_mtx,\\n'labels_mtx': labels_mtx\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = gen_data(1000, 16)\n",
    "\"\"\"\n",
    "'prop_mtx': prop_mtx,\n",
    "'op_mtx': op_mtx,\n",
    "'goal_mtx': goal_mtx,\n",
    "'labels_mtx': labels_mtx\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to tensors\n",
    "x['prop_mtx'] = torch.tensor(x['prop_mtx'], dtype=torch.float32)\n",
    "x['op_mtx'] = torch.tensor(x['op_mtx'], dtype=torch.float32)\n",
    "x['goal_mtx'] = torch.tensor(x['goal_mtx'], dtype=torch.float32)\n",
    "x['labels_mtx'] = torch.tensor(x['labels_mtx'], dtype=torch.float32)\n",
    "\n",
    "# Get dimensions\n",
    "prop_dim = x['prop_mtx'].shape[1]\n",
    "op_dim = x['op_mtx'].shape[1]\n",
    "goal_dim = x['goal_mtx'].shape[1]\n",
    "\n",
    "# Expand dimension so theres a batch of 1\n",
    "x['prop_mtx'] = x['prop_mtx'].unsqueeze(0)\n",
    "x['op_mtx'] = x['op_mtx'].unsqueeze(0)\n",
    "x['goal_mtx'] = x['goal_mtx'].unsqueeze(0)\n",
    "x['labels_mtx'] = x['labels_mtx'].unsqueeze(0)\n",
    "\n",
    "# Create model\n",
    "model = LogicModelMA(\n",
    "    in_set = {\n",
    "        'prop': prop_dim,\n",
    "        'op': op_dim,\n",
    "        'goal': goal_dim\n",
    "    },\n",
    "    embed_dim=64,\n",
    "    num_FIN_layers=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['prop', 'op', 'goal'])\n",
      "['goal', 'op', 'prop']\n"
     ]
    }
   ],
   "source": [
    "# run the model\n",
    "x_tensors = {\n",
    "    'prop': x['prop_mtx'],\n",
    "    'op': x['op_mtx'],\n",
    "    'goal': x['goal_mtx']\n",
    "}\n",
    "y = model(x_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2208, 0.6214, 0.1578],\n",
       "         [0.2219, 0.6197, 0.1585],\n",
       "         [0.2208, 0.6214, 0.1578],\n",
       "         ...,\n",
       "         [0.2217, 0.6200, 0.1583],\n",
       "         [0.2211, 0.6207, 0.1582],\n",
       "         [0.2218, 0.6195, 0.1588]]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airlift-solution",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
